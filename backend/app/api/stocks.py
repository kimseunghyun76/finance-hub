"""Stock API endpoints"""
from fastapi import APIRouter, HTTPException, status, Depends, Query
from sqlalchemy.orm import Session
from sqlalchemy import or_
from app.schemas.stock import StockInfo, StockQuote, AnalystPriceTarget, AnalystRecommendation, StockFundamentals
from app.services.data_fetcher import StockDataFetcher
from app.services.cache import (
    stock_info_cache,
    stock_quote_cache,
    analyst_targets_cache,
    STOCK_INFO_TTL,
    STOCK_QUOTE_TTL,
    ANALYST_TARGETS_TTL,
)
from app.database import get_db
from app.models.sector import StockInfo as StockInfoModel
import httpx
from bs4 import BeautifulSoup
import json
import re
import pandas as pd
import numpy as np
from datetime import datetime, timedelta

router = APIRouter()


@router.get("/{ticker}/info", response_model=StockInfo)
def get_stock_info(ticker: str, db: Session = Depends(get_db)):
    """Get stock information (company name, sector, etc.) with caching"""
    # Check cache first
    cache_key = f"stock_info_{ticker}"
    cached_info = stock_info_cache.get(cache_key)

    if cached_info:
        print(f"âœ… Returning cached info for {ticker}")
        return cached_info

    # Check database for Korean name first (optional - table may not exist)
    db_stock = None
    try:
        db_stock = db.query(StockInfoModel).filter(StockInfoModel.ticker == ticker).first()
    except Exception as e:
        # Table doesn't exist yet, skip database lookup
        print(f"âš ï¸ Database lookup skipped for {ticker}: {str(e)}")

    # Fetch from API
    stock_info = StockDataFetcher.get_stock_info(ticker)

    if not stock_info:
        raise HTTPException(
            status_code=status.HTTP_404_NOT_FOUND,
            detail=f"Stock with ticker {ticker} not found",
        )

    # If Korean stock exists in DB, use Korean name
    if db_stock and (ticker.endswith('.KS') or ticker.endswith('.KQ')):
        stock_info['name'] = db_stock.name
        if db_stock.sector:
            stock_info['sector'] = db_stock.sector.value

    # Cache the result
    stock_info_cache.set(cache_key, stock_info, STOCK_INFO_TTL)
    print(f"ğŸ’¾ Cached stock info for {ticker} (1 hour TTL)")

    return stock_info


@router.get("/{ticker}/quote", response_model=StockQuote)
def get_stock_quote(ticker: str):
    """Get current stock price with caching (5 min TTL)"""
    import yfinance as yf
    from datetime import datetime

    # Check cache first
    cache_key = f"stock_quote_{ticker}"
    cached_quote = stock_quote_cache.get(cache_key)

    if cached_quote:
        print(f"âœ… Returning cached quote for {ticker}")
        return cached_quote

    # Fetch from API
    try:
        stock = yf.Ticker(ticker)
        data = stock.history(period="1d")

        if data.empty:
            raise HTTPException(
                status_code=status.HTTP_404_NOT_FOUND,
                detail=f"No data found for ticker {ticker}",
            )

        latest = data.iloc[-1]
        previous_close = stock.info.get("previousClose", latest["Close"])

        change = latest["Close"] - previous_close
        change_percent = (change / previous_close) * 100

        quote = StockQuote(
            ticker=ticker,
            current_price=float(latest["Close"]),
            change=float(change),
            change_percent=float(change_percent),
            volume=int(latest["Volume"]),
            timestamp=datetime.now().isoformat(),
        )

        # Cache the result
        stock_quote_cache.set(cache_key, quote, STOCK_QUOTE_TTL)
        print(f"ğŸ’¾ Cached stock quote for {ticker} (5 min TTL)")

        return quote

    except Exception as e:
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail=f"Error fetching stock quote: {str(e)}",
        )


@router.get("/{ticker}/history")
def get_stock_history(ticker: str, period: str = "1mo"):
    """
    Get historical stock data with caching (1 hour TTL)

    Args:
        ticker: Stock ticker symbol
        period: Valid periods: 1d, 5d, 1mo, 3mo, 6mo, 1y, 2y, 5y, 10y, ytd, max
    """
    # Check cache first
    cache_key = f"stock_history_{ticker}_{period}"
    cached_history = stock_info_cache.get(cache_key)

    if cached_history:
        print(f"âœ… Returning cached history for {ticker} ({period})")
        return cached_history

    # Fetch from API
    df = StockDataFetcher.fetch_yahoo_finance(ticker, period)

    if df is None or df.empty:
        raise HTTPException(
            status_code=status.HTTP_404_NOT_FOUND,
            detail=f"No historical data found for ticker {ticker}",
        )

    # Convert DataFrame to list of dicts
    records = df.to_dict("records")
    result = {"ticker": ticker, "period": period, "data": records}

    # Cache the result
    stock_info_cache.set(cache_key, result, STOCK_INFO_TTL)
    print(f"ğŸ’¾ Cached stock history for {ticker} ({period}) (1 hour TTL)")

    return result


@router.get("/{ticker}/analyst-targets", response_model=AnalystPriceTarget)
def get_analyst_price_targets(ticker: str):
    """
    Get analyst price targets and recommendations with caching (1 hour TTL)

    Returns:
        - Current price
        - Analyst price targets (high, low, mean, median)
        - Recommendation summary (buy/hold/sell distribution)
        - Number of analysts covering the stock
    """
    import yfinance as yf
    from app.services.cache import yfinance_circuit_breaker

    # Check if circuit breaker is open (rate limited)
    if yfinance_circuit_breaker.is_open():
        status_obj = yfinance_circuit_breaker.get_status()
        raise HTTPException(
            status_code=status.HTTP_503_SERVICE_UNAVAILABLE,
            detail=f"Yahoo Finance API rate limited. Service will resume in {int(status_obj['seconds_remaining'])} seconds."
        )

    # Check cache first
    cache_key = f"analyst_targets_{ticker}"
    cached_targets = analyst_targets_cache.get(cache_key)

    if cached_targets:
        print(f"âœ… Returning cached analyst targets for {ticker}")
        return cached_targets

    try:
        stock = yf.Ticker(ticker)
        info = stock.info

        # Get current price
        current_price = info.get("currentPrice")
        if not current_price:
            # Fallback to regularMarketPrice
            current_price = info.get("regularMarketPrice")

        if not current_price:
            raise HTTPException(
                status_code=status.HTTP_404_NOT_FOUND,
                detail=f"No price data found for ticker {ticker}",
            )

        # Get analyst recommendations distribution
        recommendations_list = []
        try:
            recs = stock.recommendations
            if recs is not None and not recs.empty:
                # Get last 4 periods (current month and 3 previous months)
                recent_recs = recs.tail(4)
                for idx, row in recent_recs.iterrows():
                    recommendations_list.append(
                        AnalystRecommendation(
                            strong_buy=int(row.get("strongBuy", 0)),
                            buy=int(row.get("buy", 0)),
                            hold=int(row.get("hold", 0)),
                            sell=int(row.get("sell", 0)),
                            strong_sell=int(row.get("strongSell", 0)),
                            period=row.get("period", "unknown"),
                        )
                    )
        except Exception as e:
            print(f"Warning: Could not fetch recommendations: {e}")
            recommendations_list = None

        result = AnalystPriceTarget(
            ticker=ticker,
            current_price=float(current_price),
            target_high=info.get("targetHighPrice"),
            target_low=info.get("targetLowPrice"),
            target_mean=info.get("targetMeanPrice"),
            target_median=info.get("targetMedianPrice"),
            recommendation_mean=info.get("recommendationMean"),
            recommendation_key=info.get("recommendationKey"),
            number_of_analysts=info.get("numberOfAnalystOpinions"),
            recommendations=recommendations_list,
        )

        # Cache the result
        analyst_targets_cache.set(cache_key, result, ANALYST_TARGETS_TTL)
        print(f"ğŸ’¾ Cached analyst targets for {ticker} (1 hour TTL)")

        return result

    except HTTPException:
        raise
    except Exception as e:
        error_msg = str(e)

        # Check if it's a rate limit error
        if "Too Many Requests" in error_msg or "Rate limit" in error_msg or "429" in error_msg:
            yfinance_circuit_breaker.trip(f"Rate limit detected in analyst targets API for {ticker}")
            raise HTTPException(
                status_code=status.HTTP_503_SERVICE_UNAVAILABLE,
                detail=f"Yahoo Finance API rate limited. Please try again in 5 minutes.",
            )

        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail=f"Error fetching analyst data: {error_msg}",
        )


@router.get("/{ticker}/fundamentals", response_model=StockFundamentals)
def get_stock_fundamentals(ticker: str):
    """
    Get stock fundamental metrics with caching (1 hour TTL)

    Returns:
        - Valuation metrics (PER, PBR, PSR, PEG)
        - Profitability metrics (ROE, ROA, margins)
        - Growth metrics (earnings growth, revenue growth)
        - Financial health (debt ratios, liquidity ratios)
        - Dividend metrics (yield, payout ratio)
        - Risk metrics (beta)
        - Price range (52-week high/low)
    """
    import yfinance as yf
    from app.services.cache import yfinance_circuit_breaker

    # Check if circuit breaker is open (rate limited)
    if yfinance_circuit_breaker.is_open():
        status = yfinance_circuit_breaker.get_status()
        raise HTTPException(
            status_code=status.HTTP_503_SERVICE_UNAVAILABLE,
            detail=f"Yahoo Finance API rate limited. Service will resume in {int(status['seconds_remaining'])} seconds."
        )

    # Check cache first
    cache_key = f"fundamentals_{ticker}"
    cached_fundamentals = stock_info_cache.get(cache_key)

    if cached_fundamentals:
        print(f"âœ… Returning cached fundamentals for {ticker}")
        return cached_fundamentals

    try:
        stock = yf.Ticker(ticker)
        info = stock.info

        # Get current price
        current_price = info.get('currentPrice') or info.get('regularMarketPrice')

        fundamentals = StockFundamentals(
            ticker=ticker,
            # Valuation
            trailing_pe=info.get('trailingPE'),
            forward_pe=info.get('forwardPE'),
            price_to_book=info.get('priceToBook'),
            price_to_sales=info.get('priceToSalesTrailing12Months'),
            peg_ratio=info.get('pegRatio'),
            # Profitability
            return_on_equity=info.get('returnOnEquity'),
            return_on_assets=info.get('returnOnAssets'),
            profit_margins=info.get('profitMargins'),
            operating_margins=info.get('operatingMargins'),
            # Growth
            earnings_growth=info.get('earningsGrowth'),
            revenue_growth=info.get('revenueGrowth'),
            # Financial Health
            debt_to_equity=info.get('debtToEquity'),
            current_ratio=info.get('currentRatio'),
            quick_ratio=info.get('quickRatio'),
            # Dividend
            dividend_yield=info.get('dividendYield'),
            payout_ratio=info.get('payoutRatio'),
            # Risk
            beta=info.get('beta'),
            # Price Range
            fifty_two_week_high=info.get('fiftyTwoWeekHigh'),
            fifty_two_week_low=info.get('fiftyTwoWeekLow'),
            current_price=current_price,
        )

        # Cache the result
        stock_info_cache.set(cache_key, fundamentals, STOCK_INFO_TTL)
        print(f"ğŸ’¾ Cached fundamentals for {ticker} (1 hour TTL)")

        return fundamentals

    except Exception as e:
        error_msg = str(e)

        # Check if it's a rate limit error
        if "Too Many Requests" in error_msg or "Rate limit" in error_msg or "429" in error_msg:
            yfinance_circuit_breaker.trip(f"Rate limit detected in fundamentals API for {ticker}")
            raise HTTPException(
                status_code=status.HTTP_503_SERVICE_UNAVAILABLE,
                detail=f"Yahoo Finance API rate limited. Please try again in 5 minutes.",
            )

        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail=f"Error fetching fundamentals: {error_msg}",
        )


@router.get("/search")
def search_stocks(
    query: str = Query(..., min_length=1, description="Search query (ticker or company name)"),
    limit: int = Query(10, ge=1, le=50, description="Max number of results"),
    db: Session = Depends(get_db)
):
    """
    Search stocks by ticker or company name (LIKE search)

    Args:
        query: Search query (ticker symbol or company name)
        limit: Maximum number of results (default 10, max 50)

    Returns:
        List of matching stocks with ticker, name, country, market info
    """
    try:
        # Convert query to uppercase for ticker matching
        query_upper = query.upper()

        # Search database for Korean stocks first (optional - table may not exist)
        results = []
        try:
            db_stocks = db.query(StockInfoModel).filter(
                or_(
                    StockInfoModel.ticker.ilike(f"%{query_upper}%"),
                    StockInfoModel.name.ilike(f"%{query}%")
                )
            ).limit(limit).all()

            for stock in db_stocks:
                results.append({
                    "ticker": stock.ticker,
                    "name": stock.name,
                    "country": stock.country,
                    "sector": stock.sector.value if stock.sector else None,
                    "is_etf": bool(stock.is_etf),
                    "market": "KRX" if stock.ticker.endswith(('.KS', '.KQ')) else "NYSE/NASDAQ"
                })
        except Exception as e:
            # Table doesn't exist yet, skip database lookup
            print(f"âš ï¸ Database lookup skipped for search '{query}': {str(e)}")

        # If we have fewer results than limit and query looks like a US ticker, try yfinance
        if len(results) < limit and len(query) <= 5:
            import yfinance as yf

            # Check cache first for yfinance search results
            yf_cache_key = f"yf_search_{query_upper}"
            cached_yf_result = stock_info_cache.get(yf_cache_key)

            if cached_yf_result:
                print(f"âœ… Returning cached yfinance search for {query_upper}")
                if not any(r['ticker'] == query_upper for r in results):
                    results.append(cached_yf_result)
            else:
                try:
                    # Try exact ticker match from yfinance
                    stock = yf.Ticker(query_upper)
                    info = stock.info

                    # Check if we got valid data
                    if info.get('symbol') and info.get('longName'):
                        yf_result = {
                            "ticker": query_upper,
                            "name": info.get('longName', query_upper),
                            "country": "US",
                            "sector": info.get('sector'),
                            "is_etf": info.get('quoteType') == 'ETF',
                            "market": info.get('exchange', 'NYSE/NASDAQ')
                        }

                        # Cache the yfinance search result
                        stock_info_cache.set(yf_cache_key, yf_result, STOCK_INFO_TTL)
                        print(f"ğŸ’¾ Cached yfinance search for {query_upper} (1 hour TTL)")

                        # Check if already in results
                        if not any(r['ticker'] == query_upper for r in results):
                            results.append(yf_result)
                except Exception as e:
                    print(f"Could not fetch yfinance data for {query_upper}: {e}")

        return {
            "success": True,
            "query": query,
            "count": len(results),
            "results": results
        }

    except Exception as e:
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail=f"Error searching stocks: {str(e)}",
        )


@router.get("/fear-greed")
async def get_fear_greed_index():
    """Get CNN Fear & Greed Index by scraping CNN Business website"""
    cache_key = "fear_greed_index"
    cached_data = stock_info_cache.get(cache_key)

    if cached_data:
        print("âœ… Returning cached Fear & Greed Index")
        return cached_data

    try:
        # CNN Fear & Greed Index URL
        url = "https://production.dataviz.cnn.io/index/fearandgreed/graphdata"

        headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',
            'Accept': 'application/json',
            'Referer': 'https://www.cnn.com/'
        }

        async with httpx.AsyncClient(timeout=10.0) as client:
            response = await client.get(url, headers=headers)
            response.raise_for_status()

            data = response.json()

            # Extract current value
            if data and 'fear_and_greed' in data:
                current_value = data['fear_and_greed']['score']
                rating = data['fear_and_greed']['rating']
                previous_close = data.get('fear_and_greed_historical', {}).get('data', [{}])[-1].get('y', current_value) if data.get('fear_and_greed_historical') else current_value

                # Determine label based on value
                if current_value >= 75:
                    label = "Extreme Greed"
                elif current_value >= 55:
                    label = "Greed"
                elif current_value >= 45:
                    label = "Neutral"
                elif current_value >= 25:
                    label = "Fear"
                else:
                    label = "Extreme Fear"

                result = {
                    "value": current_value,
                    "label": label,
                    "rating": rating,
                    "previous_close": previous_close,
                    "timestamp": data.get('fear_and_greed', {}).get('timestamp', '')
                }

                # Cache for 1 hour
                stock_info_cache.set(cache_key, result, ttl_seconds=3600)

                return result
            else:
                raise HTTPException(
                    status_code=status.HTTP_503_SERVICE_UNAVAILABLE,
                    detail="Unable to fetch Fear & Greed Index data"
                )

    except httpx.HTTPError as e:
        print(f"HTTP error fetching Fear & Greed Index: {e}")
        raise HTTPException(
            status_code=status.HTTP_503_SERVICE_UNAVAILABLE,
            detail=f"Failed to fetch Fear & Greed Index: {str(e)}"
        )
    except Exception as e:
        print(f"Error fetching Fear & Greed Index: {e}")
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail=f"Error fetching Fear & Greed Index: {str(e)}"
        )


@router.get("/{ticker}/day-of-week-analysis")
def get_day_of_week_analysis(ticker: str, period: str = "1y"):
    """
    ìš”ì¼ë³„ ë§¤ë§¤ íŒ¨í„´ ë¶„ì„ - ìš”ì¼ë³„ í‰ê·  ìˆ˜ìµë¥ ê³¼ ê±°ë˜ëŸ‰ ë¶„ì„

    Args:
        ticker: Stock ticker symbol
        period: Analysis period (1mo, 3mo, 6mo, 1y, 2y)

    Returns:
        - ìš”ì¼ë³„ í‰ê·  ìˆ˜ìµë¥ 
        - ìš”ì¼ë³„ í‰ê·  ê±°ë˜ëŸ‰
        - ë§¤ìˆ˜ì„¸/ë§¤ë„ì„¸ê°€ ê°•í•œ ìš”ì¼
        - í†µê³„ì  ìœ ì˜ì„±
    """
    import yfinance as yf
    import pandas as pd
    import numpy as np

    cache_key = f"dow_analysis_{ticker}_{period}"
    cached_data = stock_info_cache.get(cache_key)

    if cached_data:
        print(f"âœ… Returning cached day-of-week analysis for {ticker}")
        return cached_data

    try:
        stock = yf.Ticker(ticker)
        df = stock.history(period=period)

        if df.empty:
            raise HTTPException(
                status_code=status.HTTP_404_NOT_FOUND,
                detail=f"No data found for ticker {ticker}"
            )

        # ìš”ì¼ ì¶”ê°€ (0=Monday, 6=Sunday)
        df['DayOfWeek'] = df.index.dayofweek
        df['DayName'] = df.index.day_name()
        df['Returns'] = df['Close'].pct_change() * 100

        # ìš”ì¼ë³„ í†µê³„
        day_stats = df.groupby('DayName').agg({
            'Returns': ['mean', 'std', 'count'],
            'Volume': 'mean',
            'Close': 'count'
        }).round(4)

        # ìš”ì¼ ìˆœì„œ ì •ë ¬
        day_order = ['Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday', 'Saturday', 'Sunday']
        day_names_korean = {
            'Monday': 'ì›”ìš”ì¼',
            'Tuesday': 'í™”ìš”ì¼',
            'Wednesday': 'ìˆ˜ìš”ì¼',
            'Thursday': 'ëª©ìš”ì¼',
            'Friday': 'ê¸ˆìš”ì¼',
            'Saturday': 'í† ìš”ì¼',
            'Sunday': 'ì¼ìš”ì¼'
        }

        results = []
        for day in day_order:
            if day in day_stats.index:
                avg_return = float(day_stats.loc[day, ('Returns', 'mean')])
                avg_volume = int(day_stats.loc[day, ('Volume', 'mean')])
                std_return = float(day_stats.loc[day, ('Returns', 'std')])
                count = int(day_stats.loc[day, ('Returns', 'count')])

                # ë§¤ìˆ˜ì„¸/ë§¤ë„ì„¸ íŒë‹¨
                if avg_return > 0.2:
                    sentiment = "ê°•í•œ ë§¤ìˆ˜ì„¸"
                elif avg_return > 0:
                    sentiment = "ì•½í•œ ë§¤ìˆ˜ì„¸"
                elif avg_return > -0.2:
                    sentiment = "ì•½í•œ ë§¤ë„ì„¸"
                else:
                    sentiment = "ê°•í•œ ë§¤ë„ì„¸"

                results.append({
                    'day': day,
                    'day_korean': day_names_korean[day],
                    'avg_return': avg_return,
                    'std_return': std_return,
                    'avg_volume': avg_volume,
                    'sample_count': count,
                    'sentiment': sentiment
                })

        # ìµœê³ /ìµœì € ìˆ˜ìµë¥  ìš”ì¼
        best_day = max(results, key=lambda x: x['avg_return'])
        worst_day = min(results, key=lambda x: x['avg_return'])
        highest_volume_day = max(results, key=lambda x: x['avg_volume'])

        analysis = {
            'ticker': ticker,
            'period': period,
            'day_stats': results,
            'insights': {
                'best_performing_day': {
                    'day': best_day['day_korean'],
                    'avg_return': best_day['avg_return']
                },
                'worst_performing_day': {
                    'day': worst_day['day_korean'],
                    'avg_return': worst_day['avg_return']
                },
                'highest_volume_day': {
                    'day': highest_volume_day['day_korean'],
                    'avg_volume': highest_volume_day['avg_volume']
                }
            }
        }

        # Cache for 1 day
        stock_info_cache.set(cache_key, analysis, ttl_seconds=86400)
        print(f"ğŸ’¾ Cached day-of-week analysis for {ticker} (1 day TTL)")

        return analysis

    except Exception as e:
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail=f"Error analyzing day-of-week patterns: {str(e)}"
        )


@router.get("/{ticker}/technical-indicators")
def get_technical_indicators(ticker: str):
    """
    ê¸°ìˆ ì  ì§€í‘œ ë¶„ì„ - RSI, MACD, ë³¼ë¦°ì € ë°´ë“œ, ì´ë™í‰ê· ì„ 

    Returns:
        - RSI (ìƒëŒ€ê°•ë„ì§€ìˆ˜)
        - MACD (ì´ë™í‰ê· ìˆ˜ë ´í™•ì‚°)
        - ë³¼ë¦°ì € ë°´ë“œ (ìƒë‹¨/ì¤‘ê°„/í•˜ë‹¨)
        - ì´ë™í‰ê· ì„  (5, 20, 60, 120ì¼)
        - ë§¤ë§¤ ì‹ í˜¸
    """
    import yfinance as yf
    import pandas as pd
    import numpy as np

    cache_key = f"tech_indicators_{ticker}"
    cached_data = stock_quote_cache.get(cache_key)

    if cached_data:
        print(f"âœ… Returning cached technical indicators for {ticker}")
        return cached_data

    try:
        stock = yf.Ticker(ticker)
        df = stock.history(period="6mo")

        if df.empty:
            raise HTTPException(
                status_code=status.HTTP_404_NOT_FOUND,
                detail=f"No data found for ticker {ticker}"
            )

        # RSI ê³„ì‚°
        def calculate_rsi(data, period=14):
            delta = data.diff()
            gain = (delta.where(delta > 0, 0)).rolling(window=period).mean()
            loss = (-delta.where(delta < 0, 0)).rolling(window=period).mean()
            rs = gain / loss
            rsi = 100 - (100 / (1 + rs))
            return rsi

        # MACD ê³„ì‚°
        def calculate_macd(data):
            exp1 = data.ewm(span=12, adjust=False).mean()
            exp2 = data.ewm(span=26, adjust=False).mean()
            macd = exp1 - exp2
            signal = macd.ewm(span=9, adjust=False).mean()
            histogram = macd - signal
            return macd, signal, histogram

        # ë³¼ë¦°ì € ë°´ë“œ ê³„ì‚°
        def calculate_bollinger_bands(data, period=20):
            sma = data.rolling(window=period).mean()
            std = data.rolling(window=period).std()
            upper = sma + (std * 2)
            lower = sma - (std * 2)
            return upper, sma, lower

        # ì§€í‘œ ê³„ì‚°
        df['RSI'] = calculate_rsi(df['Close'])
        df['MACD'], df['Signal'], df['Histogram'] = calculate_macd(df['Close'])
        df['BB_Upper'], df['BB_Middle'], df['BB_Lower'] = calculate_bollinger_bands(df['Close'])
        df['MA5'] = df['Close'].rolling(window=5).mean()
        df['MA20'] = df['Close'].rolling(window=20).mean()
        df['MA60'] = df['Close'].rolling(window=60).mean()
        df['MA120'] = df['Close'].rolling(window=120).mean()

        # ìµœì‹  ë°ì´í„°
        latest = df.iloc[-1]
        current_price = float(latest['Close'])

        # ë§¤ë§¤ ì‹ í˜¸ ìƒì„±
        signals = []

        # RSI ì‹ í˜¸
        rsi = float(latest['RSI'])
        if rsi < 30:
            signals.append({'type': 'RSI', 'signal': 'BUY', 'strength': 'STRONG', 'reason': f'ê³¼ë§¤ë„ êµ¬ê°„ (RSI: {rsi:.1f})'})
        elif rsi > 70:
            signals.append({'type': 'RSI', 'signal': 'SELL', 'strength': 'STRONG', 'reason': f'ê³¼ë§¤ìˆ˜ êµ¬ê°„ (RSI: {rsi:.1f})'})

        # MACD ì‹ í˜¸
        macd = float(latest['MACD'])
        signal_line = float(latest['Signal'])
        if macd > signal_line and df.iloc[-2]['MACD'] <= df.iloc[-2]['Signal']:
            signals.append({'type': 'MACD', 'signal': 'BUY', 'strength': 'MODERATE', 'reason': 'MACD ê³¨ë“ í¬ë¡œìŠ¤'})
        elif macd < signal_line and df.iloc[-2]['MACD'] >= df.iloc[-2]['Signal']:
            signals.append({'type': 'MACD', 'signal': 'SELL', 'strength': 'MODERATE', 'reason': 'MACD ë°ë“œí¬ë¡œìŠ¤'})

        # ë³¼ë¦°ì € ë°´ë“œ ì‹ í˜¸
        bb_upper = float(latest['BB_Upper'])
        bb_lower = float(latest['BB_Lower'])
        if current_price <= bb_lower:
            signals.append({'type': 'BOLLINGER', 'signal': 'BUY', 'strength': 'MODERATE', 'reason': 'ë³¼ë¦°ì € í•˜ë‹¨ ì´íƒˆ'})
        elif current_price >= bb_upper:
            signals.append({'type': 'BOLLINGER', 'signal': 'SELL', 'strength': 'MODERATE', 'reason': 'ë³¼ë¦°ì € ìƒë‹¨ ì´íƒˆ'})

        # ì´ë™í‰ê· ì„  ì‹ í˜¸
        ma5 = float(latest['MA5'])
        ma20 = float(latest['MA20'])
        if ma5 > ma20 and df.iloc[-2]['MA5'] <= df.iloc[-2]['MA20']:
            signals.append({'type': 'MA', 'signal': 'BUY', 'strength': 'WEAK', 'reason': '5ì¼ì„  20ì¼ì„  ê³¨ë“ í¬ë¡œìŠ¤'})
        elif ma5 < ma20 and df.iloc[-2]['MA5'] >= df.iloc[-2]['MA20']:
            signals.append({'type': 'MA', 'signal': 'SELL', 'strength': 'WEAK', 'reason': '5ì¼ì„  20ì¼ì„  ë°ë“œí¬ë¡œìŠ¤'})

        result = {
            'ticker': ticker,
            'current_price': current_price,
            'indicators': {
                'rsi': {
                    'value': rsi,
                    'signal': 'BUY' if rsi < 30 else 'SELL' if rsi > 70 else 'NEUTRAL',
                    'level': 'ê³¼ë§¤ë„' if rsi < 30 else 'ê³¼ë§¤ìˆ˜' if rsi > 70 else 'ì¤‘ë¦½'
                },
                'macd': {
                    'macd': macd,
                    'signal': signal_line,
                    'histogram': float(latest['Histogram']),
                    'trend': 'BULLISH' if macd > signal_line else 'BEARISH'
                },
                'bollinger_bands': {
                    'upper': bb_upper,
                    'middle': float(latest['BB_Middle']),
                    'lower': bb_lower,
                    'position': 'ìƒë‹¨ê¶Œ' if current_price > bb_upper else 'í•˜ë‹¨ê¶Œ' if current_price < bb_lower else 'ì¤‘ê°„ê¶Œ'
                },
                'moving_averages': {
                    'ma5': ma5,
                    'ma20': ma20,
                    'ma60': float(latest['MA60']) if not pd.isna(latest['MA60']) else None,
                    'ma120': float(latest['MA120']) if not pd.isna(latest['MA120']) else None
                }
            },
            'signals': signals,
            'overall_signal': 'BUY' if len([s for s in signals if s['signal'] == 'BUY']) > len([s for s in signals if s['signal'] == 'SELL']) else 'SELL' if len([s for s in signals if s['signal'] == 'SELL']) > len([s for s in signals if s['signal'] == 'BUY']) else 'NEUTRAL'
        }

        # Cache for 5 minutes
        stock_quote_cache.set(cache_key, result, STOCK_QUOTE_TTL)
        print(f"ğŸ’¾ Cached technical indicators for {ticker} (5 min TTL)")

        return result

    except Exception as e:
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail=f"Error calculating technical indicators: {str(e)}"
        )


@router.get("/{ticker}/events")
def get_stock_events(ticker: str):
    """
    ì£¼ìš” ì£¼ì‹ ì´ë²¤íŠ¸ ìº˜ë¦°ë” - ë°°ë‹¹ë½ì¼, ì‹¤ì ë°œí‘œì¼, ì˜µì…˜ë§Œê¸°ì¼

    Returns:
        - ë‹¤ìŒ ë°°ë‹¹ë½ì¼ ë° ë°°ë‹¹ê¸ˆ
        - ë‹¤ìŒ ì‹¤ì ë°œí‘œì¼
        - ì˜µì…˜ë§Œê¸°ì¼ (ì›”ë³„)
        - ì´ë²¤íŠ¸ ì„íŒ©íŠ¸ ì˜ˆì¸¡
    """
    import yfinance as yf
    from datetime import datetime, timedelta

    cache_key = f"stock_events_{ticker}"
    cached_data = stock_info_cache.get(cache_key)

    if cached_data:
        print(f"âœ… Returning cached stock events for {ticker}")
        return cached_data

    try:
        stock = yf.Ticker(ticker)
        info = stock.info
        calendar = stock.calendar

        events = []

        # ë°°ë‹¹ ì´ë²¤íŠ¸
        if info.get('dividendDate'):
            div_date = datetime.fromtimestamp(info['dividendDate'])
            dividend_amount = info.get('dividendRate', info.get('trailingAnnualDividendRate', 0))
            events.append({
                'type': 'ë°°ë‹¹ë½ì¼',
                'date': div_date.strftime('%Y-%m-%d'),
                'impact': 'MODERATE',
                'description': f'ë°°ë‹¹ê¸ˆ: ${dividend_amount:.2f}',
                'days_until': (div_date - datetime.now()).days
            })

        # ì‹¤ì ë°œí‘œì¼
        if calendar is not None and 'Earnings Date' in calendar:
            earnings_dates = calendar['Earnings Date']
            if len(earnings_dates) > 0:
                earnings_date = pd.to_datetime(earnings_dates[0])
                events.append({
                    'type': 'ì‹¤ì ë°œí‘œ',
                    'date': earnings_date.strftime('%Y-%m-%d'),
                    'impact': 'HIGH',
                    'description': 'ë¶„ê¸° ì‹¤ì  ë°œí‘œ ì˜ˆì •',
                    'days_until': (earnings_date - datetime.now()).days
                })

        # ì˜µì…˜ë§Œê¸°ì¼ (ë¯¸êµ­ ì£¼ì‹: ë§¤ì›” ì…‹ì§¸ ê¸ˆìš”ì¼)
        def get_next_option_expiry():
            today = datetime.now()
            year = today.year
            month = today.month

            expirations = []
            for i in range(3):  # ë‹¤ìŒ 3ê°œì›”
                target_month = month + i
                target_year = year
                if target_month > 12:
                    target_month -= 12
                    target_year += 1

                # í•´ë‹¹ ì›”ì˜ ì²« ë‚ 
                first_day = datetime(target_year, target_month, 1)
                # ì²« ê¸ˆìš”ì¼ ì°¾ê¸°
                days_until_friday = (4 - first_day.weekday()) % 7
                first_friday = first_day + timedelta(days=days_until_friday)
                # ì…‹ì§¸ ê¸ˆìš”ì¼
                third_friday = first_friday + timedelta(weeks=2)

                if third_friday > today:
                    expirations.append(third_friday)

            return expirations

        option_dates = get_next_option_expiry()
        for opt_date in option_dates:
            events.append({
                'type': 'ì˜µì…˜ë§Œê¸°ì¼',
                'date': opt_date.strftime('%Y-%m-%d'),
                'impact': 'MODERATE',
                'description': 'ì›”ê°„ ì˜µì…˜ ë§Œê¸°ì¼ (ë³€ë™ì„± ì¦ê°€ ê°€ëŠ¥)',
                'days_until': (opt_date - datetime.now()).days
            })

        # ì´ë²¤íŠ¸ë¥¼ ë‚ ì§œìˆœìœ¼ë¡œ ì •ë ¬
        events.sort(key=lambda x: x['days_until'])

        result = {
            'ticker': ticker,
            'events': events,
            'next_major_event': events[0] if events else None
        }

        # Cache for 1 day
        stock_info_cache.set(cache_key, result, ttl_seconds=86400)
        print(f"ğŸ’¾ Cached stock events for {ticker} (1 day TTL)")

        return result

    except Exception as e:
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail=f"Error fetching stock events: {str(e)}"
        )


@router.get("/exchange-rate-impact")
def get_exchange_rate_impact():
    """
    USD/KRW í™˜ìœ¨ ì˜í–¥ ë¶„ì„ - í™˜ìœ¨ì´ í•œêµ­ ì¦ì‹œì— ë¯¸ì¹˜ëŠ” ì˜í–¥

    Returns:
        - í˜„ì¬ USD/KRW í™˜ìœ¨
        - í™˜ìœ¨ ë³€í™” ì¶”ì´
        - ì½”ìŠ¤í”¼ ìƒê´€ê´€ê³„
        - íˆ¬ì ê°€ì´ë“œ
    """
    import yfinance as yf
    import pandas as pd
    import numpy as np

    cache_key = "exchange_rate_impact"
    cached_data = stock_info_cache.get(cache_key)

    if cached_data:
        print("âœ… Returning cached exchange rate impact analysis")
        return cached_data

    try:
        # USD/KRW í™˜ìœ¨ ë°ì´í„°
        usdkrw = yf.Ticker("KRW=X")
        usdkrw_data = usdkrw.history(period="3mo")

        # ì½”ìŠ¤í”¼ ë°ì´í„°
        kospi = yf.Ticker("^KS11")
        kospi_data = kospi.history(period="3mo")

        if usdkrw_data.empty or kospi_data.empty:
            raise HTTPException(
                status_code=status.HTTP_404_NOT_FOUND,
                detail="Exchange rate or KOSPI data not available"
            )

        # í˜„ì¬ í™˜ìœ¨
        current_rate = float(usdkrw_data['Close'].iloc[-1])
        prev_rate = float(usdkrw_data['Close'].iloc[-2])
        rate_change = current_rate - prev_rate
        rate_change_pct = (rate_change / prev_rate) * 100

        # í™˜ìœ¨ í†µê³„
        avg_3m = float(usdkrw_data['Close'].mean())
        max_3m = float(usdkrw_data['Close'].max())
        min_3m = float(usdkrw_data['Close'].min())

        # í™˜ìœ¨ ìˆ˜ì¤€ íŒë‹¨
        if current_rate > avg_3m + (max_3m - min_3m) * 0.25:
            rate_level = "ê³ í™˜ìœ¨"
            rate_description = "ì›í™” ì•½ì„¸ (ë‹¬ëŸ¬ ê°•ì„¸)"
        elif current_rate < avg_3m - (max_3m - min_3m) * 0.25:
            rate_level = "ì €í™˜ìœ¨"
            rate_description = "ì›í™” ê°•ì„¸ (ë‹¬ëŸ¬ ì•½ì„¸)"
        else:
            rate_level = "ì¤‘ë¦½"
            rate_description = "í™˜ìœ¨ ì•ˆì •"

        # ìƒê´€ê´€ê³„ ë¶„ì„ (í™˜ìœ¨ ìƒìŠ¹ vs ì½”ìŠ¤í”¼ í•˜ë½)
        # ë‘ ë°ì´í„°ì˜ ê³µí†µ ë‚ ì§œë§Œ ì‚¬ìš©
        merged = pd.merge(
            usdkrw_data['Close'].rename('USDKRW'),
            kospi_data['Close'].rename('KOSPI'),
            left_index=True,
            right_index=True,
            how='inner'
        )

        if len(merged) > 10:
            correlation = merged['USDKRW'].corr(merged['KOSPI'])
        else:
            correlation = 0

        # íˆ¬ì ê°€ì´ë“œ ìƒì„±
        guides = []

        if rate_level == "ê³ í™˜ìœ¨":
            guides.append({
                'category': 'ìˆ˜ì¶œì£¼',
                'recommendation': 'POSITIVE',
                'reason': 'ê³ í™˜ìœ¨ì€ ìˆ˜ì¶œ ê¸°ì—…ì— ìœ ë¦¬ (í™˜ì°¨ìµ ë°œìƒ)'
            })
            guides.append({
                'category': 'ìˆ˜ì…ì£¼',
                'recommendation': 'NEGATIVE',
                'reason': 'ê³ í™˜ìœ¨ì€ ìˆ˜ì… ë¹„ìš© ì¦ê°€ë¡œ ìˆ˜ì… ê¸°ì—…ì— ë¶ˆë¦¬'
            })
            guides.append({
                'category': 'ì™¸êµ­ì¸ íˆ¬ì',
                'recommendation': 'NEUTRAL',
                'reason': 'ë‹¬ëŸ¬ ê°•ì„¸ ì‹œ ì™¸êµ­ì¸ ìê¸ˆ ìœ ì¶œ ê°€ëŠ¥ì„±'
            })
        elif rate_level == "ì €í™˜ìœ¨":
            guides.append({
                'category': 'ìˆ˜ì¶œì£¼',
                'recommendation': 'NEGATIVE',
                'reason': 'ì €í™˜ìœ¨ì€ ìˆ˜ì¶œ ê¸°ì—…ì˜ ê°€ê²© ê²½ìŸë ¥ ì•½í™”'
            })
            guides.append({
                'category': 'ìˆ˜ì…ì£¼',
                'recommendation': 'POSITIVE',
                'reason': 'ì €í™˜ìœ¨ì€ ìˆ˜ì… ë¹„ìš© ê°ì†Œë¡œ ìˆ˜ì… ê¸°ì—…ì— ìœ ë¦¬'
            })
            guides.append({
                'category': 'ì™¸êµ­ì¸ íˆ¬ì',
                'recommendation': 'POSITIVE',
                'reason': 'ì›í™” ê°•ì„¸ ì‹œ ì™¸êµ­ì¸ ìê¸ˆ ìœ ì… ê°€ëŠ¥ì„±'
            })
        else:
            guides.append({
                'category': 'ì‹œì¥ ì „ë°˜',
                'recommendation': 'NEUTRAL',
                'reason': 'í™˜ìœ¨ ì•ˆì •ìœ¼ë¡œ ì‹œì¥ ë³€ë™ì„± ì œí•œì '
            })

        # ì—…ì¢…ë³„ ì˜í–¥ë„
        sector_impacts = [
            {
                'sector': 'ìë™ì°¨/ë¶€í’ˆ',
                'export_ratio': 'HIGH',
                'impact': 'POSITIVE' if rate_level == "ê³ í™˜ìœ¨" else 'NEGATIVE' if rate_level == "ì €í™˜ìœ¨" else 'NEUTRAL'
            },
            {
                'sector': 'ë°˜ë„ì²´',
                'export_ratio': 'HIGH',
                'impact': 'POSITIVE' if rate_level == "ê³ í™˜ìœ¨" else 'NEGATIVE' if rate_level == "ì €í™˜ìœ¨" else 'NEUTRAL'
            },
            {
                'sector': 'ì •ìœ /í™”í•™',
                'export_ratio': 'MEDIUM',
                'impact': 'NEUTRAL' if rate_level != "ê³ í™˜ìœ¨" else 'POSITIVE'
            },
            {
                'sector': 'ìœ í†µ/ì‹í’ˆ',
                'export_ratio': 'LOW',
                'impact': 'NEGATIVE' if rate_level == "ê³ í™˜ìœ¨" else 'POSITIVE' if rate_level == "ì €í™˜ìœ¨" else 'NEUTRAL'
            }
        ]

        result = {
            'current_rate': current_rate,
            'change': rate_change,
            'change_percent': rate_change_pct,
            'rate_level': rate_level,
            'rate_description': rate_description,
            'statistics': {
                'avg_3month': avg_3m,
                'max_3month': max_3m,
                'min_3month': min_3m
            },
            'kospi_correlation': float(correlation),
            'correlation_description': 'ìŒì˜ ìƒê´€ê´€ê³„ (í™˜ìœ¨ â†‘ â†’ ì½”ìŠ¤í”¼ â†“)' if correlation < -0.3 else 'ì–‘ì˜ ìƒê´€ê´€ê³„ (í™˜ìœ¨ â†‘ â†’ ì½”ìŠ¤í”¼ â†‘)' if correlation > 0.3 else 'ì•½í•œ ìƒê´€ê´€ê³„',
            'investment_guides': guides,
            'sector_impacts': sector_impacts
        }

        # Cache for 5 minutes
        stock_info_cache.set(cache_key, result, ttl_seconds=300)
        print("ğŸ’¾ Cached exchange rate impact analysis (5 min TTL)")

        return result

    except Exception as e:
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail=f"Error analyzing exchange rate impact: {str(e)}"
        )


@router.get("/{ticker}/risk-score")
def get_risk_score(ticker: str):
    """
    ì¢…í•© ë¦¬ìŠ¤í¬ ì ìˆ˜ ê³„ì‚° - ìì²´ ê°œë°œ ë¦¬ìŠ¤í¬ í‰ê°€ ëª¨ë¸

    Returns:
        - ì¢…í•© ë¦¬ìŠ¤í¬ ì ìˆ˜ (0-100, ë‚®ì„ìˆ˜ë¡ ì•ˆì „)
        - ìœ„í—˜ë„ ë“±ê¸‰ (ë§¤ìš° ë‚®ìŒ/ë‚®ìŒ/ë³´í†µ/ë†’ìŒ/ë§¤ìš° ë†’ìŒ)
        - ì„¸ë¶€ ë¦¬ìŠ¤í¬ ìš”ì†Œ (ë³€ë™ì„±, ë² íƒ€, ë‚™í­, ìœ ë™ì„±)
        - íˆ¬ìì ì„±í–¥ ë§¤ì¹­
    """
    import yfinance as yf
    import pandas as pd
    import numpy as np

    cache_key = f"risk_score_{ticker}"
    cached_data = stock_quote_cache.get(cache_key)

    if cached_data:
        print(f"âœ… Returning cached risk score for {ticker}")
        return cached_data

    try:
        stock = yf.Ticker(ticker)
        info = stock.info
        df = stock.history(period="1y")

        if df.empty:
            raise HTTPException(
                status_code=status.HTTP_404_NOT_FOUND,
                detail=f"No data found for ticker {ticker}"
            )

        # 1. ë³€ë™ì„± ë¦¬ìŠ¤í¬ (0-30ì )
        returns = df['Close'].pct_change().dropna()
        volatility = returns.std() * np.sqrt(252) * 100  # ì—°ê°„ ë³€ë™ì„± (%)
        volatility_score = min(volatility * 0.5, 30)  # 60% ë³€ë™ì„± = 30ì 

        # 2. ë² íƒ€ ë¦¬ìŠ¤í¬ (0-25ì )
        beta = info.get('beta', 1.0)
        if beta is None:
            beta = 1.0
        beta_score = min(abs(beta - 1.0) * 25, 25)  # ë² íƒ€ê°€ 1ì—ì„œ ë©€ìˆ˜ë¡ ìœ„í—˜

        # 3. ìµœëŒ€ë‚™í­ ë¦¬ìŠ¤í¬ (0-30ì )
        cummax = df['Close'].cummax()
        drawdown = ((df['Close'] - cummax) / cummax * 100).min()
        max_drawdown = abs(drawdown)
        drawdown_score = min(max_drawdown * 0.6, 30)  # 50% ë‚™í­ = 30ì 

        # 4. ìœ ë™ì„± ë¦¬ìŠ¤í¬ (0-15ì )
        avg_volume = df['Volume'].mean()
        if avg_volume < 100000:
            liquidity_score = 15
        elif avg_volume < 500000:
            liquidity_score = 10
        elif avg_volume < 1000000:
            liquidity_score = 5
        else:
            liquidity_score = 0

        # ì¢…í•© ë¦¬ìŠ¤í¬ ì ìˆ˜
        total_risk_score = volatility_score + beta_score + drawdown_score + liquidity_score

        # ìœ„í—˜ë„ ë“±ê¸‰
        if total_risk_score < 20:
            risk_level = "ë§¤ìš° ë‚®ìŒ"
            risk_color = "green"
            investor_match = "ë³´ìˆ˜ì  íˆ¬ìì"
        elif total_risk_score < 40:
            risk_level = "ë‚®ìŒ"
            risk_color = "lightgreen"
            investor_match = "ì•ˆì • ì¶”êµ¬ íˆ¬ìì"
        elif total_risk_score < 60:
            risk_level = "ë³´í†µ"
            risk_color = "yellow"
            investor_match = "ê· í˜• íˆ¬ìì"
        elif total_risk_score < 80:
            risk_level = "ë†’ìŒ"
            risk_color = "orange"
            investor_match = "ì ê·¹ì  íˆ¬ìì"
        else:
            risk_level = "ë§¤ìš° ë†’ìŒ"
            risk_color = "red"
            investor_match = "ê³µê²©ì  íˆ¬ìì"

        result = {
            'ticker': ticker,
            'risk_score': round(total_risk_score, 2),
            'risk_level': risk_level,
            'risk_color': risk_color,
            'investor_match': investor_match,
            'risk_breakdown': {
                'volatility': {
                    'score': round(volatility_score, 2),
                    'value': round(volatility, 2),
                    'description': f'ì—°ê°„ ë³€ë™ì„± {volatility:.1f}%'
                },
                'beta': {
                    'score': round(beta_score, 2),
                    'value': round(beta, 2),
                    'description': f'ì‹œì¥ ëŒ€ë¹„ ë¯¼ê°ë„ {beta:.2f}'
                },
                'max_drawdown': {
                    'score': round(drawdown_score, 2),
                    'value': round(max_drawdown, 2),
                    'description': f'ìµœëŒ€ ë‚™í­ {max_drawdown:.1f}%'
                },
                'liquidity': {
                    'score': round(liquidity_score, 2),
                    'value': int(avg_volume),
                    'description': f'í‰ê·  ê±°ë˜ëŸ‰ {int(avg_volume):,}ì£¼'
                }
            },
            'recommendation': _get_risk_recommendation(risk_level)
        }

        # Cache for 1 hour
        stock_quote_cache.set(cache_key, result, ttl_seconds=3600)
        print(f"ğŸ’¾ Cached risk score for {ticker} (1 hour TTL)")

        return result

    except Exception as e:
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail=f"Error calculating risk score: {str(e)}"
        )


def _get_risk_recommendation(risk_level: str) -> str:
    """ë¦¬ìŠ¤í¬ ë“±ê¸‰ë³„ íˆ¬ì ê¶Œê³ ì‚¬í•­"""
    recommendations = {
        "ë§¤ìš° ë‚®ìŒ": "ì•ˆì •ì ì¸ íˆ¬ì ëŒ€ìƒ. ì¥ê¸° ë³´ìœ ì— ì í•©í•˜ë©°, í¬íŠ¸í´ë¦¬ì˜¤ì˜ í•µì‹¬ ìì‚°ìœ¼ë¡œ í™œìš© ê°€ëŠ¥í•©ë‹ˆë‹¤.",
        "ë‚®ìŒ": "ë¹„êµì  ì•ˆì „í•œ íˆ¬ìì²˜. ì ì ˆí•œ ìˆ˜ìµê³¼ ì•ˆì •ì„±ì„ ê· í˜•ìˆê²Œ ì œê³µí•©ë‹ˆë‹¤.",
        "ë³´í†µ": "ì¤‘ê°„ ìˆ˜ì¤€ì˜ ë¦¬ìŠ¤í¬. ë¶„ì‚° íˆ¬ìë¥¼ í†µí•´ ë¦¬ìŠ¤í¬ ê´€ë¦¬ê°€ í•„ìš”í•©ë‹ˆë‹¤.",
        "ë†’ìŒ": "ë†’ì€ ë³€ë™ì„± ì£¼ì˜. ë‹¨ê¸° íˆ¬ì ë˜ëŠ” ì†Œì•¡ ë¹„ì¤‘ìœ¼ë¡œ ì ‘ê·¼í•˜ì„¸ìš”.",
        "ë§¤ìš° ë†’ìŒ": "ë§¤ìš° ë†’ì€ ë¦¬ìŠ¤í¬. íˆ¬ì ê²½í—˜ì´ í’ë¶€í•˜ê³  ì†ì‹¤ ê°ë‚´ ëŠ¥ë ¥ì´ ìˆëŠ” íˆ¬ììë§Œ ê³ ë ¤í•˜ì„¸ìš”."
    }
    return recommendations.get(risk_level, "íˆ¬ìì— ì£¼ì˜ê°€ í•„ìš”í•©ë‹ˆë‹¤.")


@router.get("/{ticker}/anomaly-detection")
def detect_anomalies(ticker: str):
    """
    ì´ìƒ ê±°ë˜ íƒì§€ ì‹œìŠ¤í…œ - í†µê³„ ê¸°ë°˜ ì´ìƒ ì§•í›„ ê°ì§€

    Returns:
        - ê°€ê²© ì´ìƒ ì§•í›„ (Z-score ê¸°ë°˜)
        - ê±°ë˜ëŸ‰ ì´ìƒ ì§•í›„
        - ê¸‰ë“±/ê¸‰ë½ ì•Œë¦¼
        - íŒ¨í„´ ì´íƒˆ ê°ì§€
    """
    import yfinance as yf
    import pandas as pd
    import numpy as np
    from datetime import datetime, timedelta

    cache_key = f"anomaly_{ticker}"
    cached_data = stock_quote_cache.get(cache_key)

    if cached_data:
        print(f"âœ… Returning cached anomaly detection for {ticker}")
        return cached_data

    try:
        stock = yf.Ticker(ticker)
        df = stock.history(period="3mo")

        if df.empty or len(df) < 20:
            raise HTTPException(
                status_code=status.HTTP_404_NOT_FOUND,
                detail=f"Insufficient data for ticker {ticker}"
            )

        # ì¼ì¼ ìˆ˜ìµë¥  ê³„ì‚°
        df['Returns'] = df['Close'].pct_change() * 100

        # 1. ê°€ê²© ì´ìƒ ì§•í›„ (Z-score ë¶„ì„)
        price_mean = df['Close'].mean()
        price_std = df['Close'].std()
        latest_price = df['Close'].iloc[-1]
        price_zscore = (latest_price - price_mean) / price_std if price_std > 0 else 0

        price_anomaly = {
            'detected': bool(abs(price_zscore) > 2),
            'zscore': float(round(price_zscore, 2)),
            'type': 'ê¸‰ë“±' if price_zscore > 2 else 'ê¸‰ë½' if price_zscore < -2 else 'ì •ìƒ',
            'severity': 'ë†’ìŒ' if abs(price_zscore) > 3 else 'ë³´í†µ' if abs(price_zscore) > 2 else 'ë‚®ìŒ'
        }

        # 2. ê±°ë˜ëŸ‰ ì´ìƒ ì§•í›„
        volume_mean = df['Volume'].mean()
        volume_std = df['Volume'].std()
        latest_volume = df['Volume'].iloc[-1]
        volume_zscore = (latest_volume - volume_mean) / volume_std if volume_std > 0 else 0

        volume_anomaly = {
            'detected': bool(volume_zscore > 2),
            'zscore': float(round(volume_zscore, 2)),
            'type': 'ê±°ë˜ëŸ‰ ê¸‰ì¦' if volume_zscore > 2 else 'ì •ìƒ',
            'volume_vs_avg': float(round((latest_volume / volume_mean - 1) * 100, 1) if volume_mean > 0 else 0)
        }

        # 3. ê¸‰ë“±/ê¸‰ë½ ê°ì§€ (ì¼ì¼ ìˆ˜ìµë¥ )
        returns_mean = df['Returns'].mean()
        returns_std = df['Returns'].std()
        latest_return = df['Returns'].iloc[-1]
        return_zscore = (latest_return - returns_mean) / returns_std if returns_std > 0 else 0

        price_movement = {
            'detected': bool(abs(return_zscore) > 2),
            'daily_return': float(round(latest_return, 2)),
            'zscore': float(round(return_zscore, 2)),
            'type': 'ê¸‰ë“±' if return_zscore > 2 else 'ê¸‰ë½' if return_zscore < -2 else 'ì •ìƒ'
        }

        # 4. ì—°ì† ìƒìŠ¹/í•˜ë½ ê°ì§€
        consecutive_up = 0
        consecutive_down = 0
        for ret in df['Returns'].iloc[-10:]:
            if ret > 0:
                consecutive_up += 1
                consecutive_down = 0
            elif ret < 0:
                consecutive_down += 1
                consecutive_up = 0
            else:
                break

        pattern = {
            'consecutive_up_days': consecutive_up,
            'consecutive_down_days': consecutive_down,
            'pattern_alert': consecutive_up >= 5 or consecutive_down >= 5,
            'pattern_type': f'{consecutive_up}ì¼ ì—°ì† ìƒìŠ¹' if consecutive_up >= 3 else f'{consecutive_down}ì¼ ì—°ì† í•˜ë½' if consecutive_down >= 3 else 'ë³€ë™ì„± ì¥ì„¸'
        }

        # ì¢…í•© ì´ìƒ ê°ì§€
        anomalies_detected = []
        if price_anomaly['detected']:
            anomalies_detected.append(f"ê°€ê²© {price_anomaly['type']}")
        if volume_anomaly['detected']:
            anomalies_detected.append("ê±°ë˜ëŸ‰ ê¸‰ì¦")
        if price_movement['detected']:
            anomalies_detected.append(f"ì¼ì¼ ìˆ˜ìµë¥  {price_movement['type']}")
        if pattern['pattern_alert']:
            anomalies_detected.append(pattern['pattern_type'])

        overall_status = "ì´ìƒ ê°ì§€" if anomalies_detected else "ì •ìƒ"

        result = {
            'ticker': ticker,
            'timestamp': datetime.now().isoformat(),
            'overall_status': overall_status,
            'anomalies': anomalies_detected,
            'analysis': {
                'price_anomaly': price_anomaly,
                'volume_anomaly': volume_anomaly,
                'price_movement': price_movement,
                'pattern': pattern
            },
            'alert_level': 'ë†’ìŒ' if len(anomalies_detected) >= 3 else 'ë³´í†µ' if len(anomalies_detected) >= 1 else 'ë‚®ìŒ'
        }

        # Cache for 5 minutes
        stock_quote_cache.set(cache_key, result, ttl_seconds=300)
        print(f"ğŸ’¾ Cached anomaly detection for {ticker} (5 min TTL)")

        return result

    except Exception as e:
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail=f"Error detecting anomalies: {str(e)}"
        )
